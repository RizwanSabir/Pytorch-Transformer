{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "        # It get the src and tgt sentence of one line\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "        # this generate  [1,2,3,4] tokens generated\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            # only the rows will be 0 that has the padding token [pad] \n",
    "            \n",
    "             #        i*64    [PAD]*64    a*64    cat*64\n",
    "            # i*64    [0.2  0.3  0.1  0.4]\n",
    "            # am*64   [0.1  0.4  0.2  0.3]\n",
    "            # [Pad]*64    [0.1  0.2  0.3  0.4]\n",
    "            # cat*64  [0.3  0.2  0.1  0.4]\n",
    "            \n",
    "            # so the mask[1,1,0,1] will give the 0 to the third row  \n",
    "            \n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \n",
    "            #        i*64    [PAD]*64    a*64    cat*64\n",
    "            # i*64    [0.2  0.3  0.1  0.4]\n",
    "            # am*64   [0.1  0.4  0.2  0.3]\n",
    "            # [Pad]*64    [0.1  0.2  0.3  0.4]\n",
    "            # cat*64  [0.3  0.2  0.1  0.4]\n",
    "            \n",
    "            # so the mask[1,1,0,1] will give the 0 to the third row \n",
    "            \n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), \n",
    "\n",
    " \n",
    " It tell where there is token that is not a pad token add 1\n",
    " It tell where there is token that is equal a pad token add 0\n",
    " so the encoder mask will be \n",
    "\n",
    " [1,1,1,1,1,1,0,0,0,0]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](PicturesPart2/Billingual.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excellent question! The BilingualDataset class we've been discussing is a custom Dataset \n",
    "# class that's designed to work with PyTorch's DataLoader. Let's clarify how these \n",
    "# two components fit together.\n",
    "\n",
    "# 1. Dataset (Like BilingualDataset):\n",
    "\n",
    "# Purpose: The Dataset class is responsible for accessing and preparing \n",
    "# individual data samples. It's like an interface to your data source \n",
    "# (e.g., a list of sentences, a file, a database).\n",
    "\n",
    "# Key Methods:\n",
    "\n",
    "# __init__(self, ...): This is the constructor. It initializes the\n",
    "# dataset, usually by loading data from a file or other source. In\n",
    "# our case, it receives the raw dataset, tokenizers, language codes, and sequence length.\n",
    "\n",
    "# __len__(self): This method returns the total number of samples\n",
    "# in the dataset. The DataLoader uses this to know how many batches to create.\n",
    "\n",
    "# __getitem__(self, idx): This is the most important method.\n",
    "# Given an index idx, it returns the idx-th data sample as a dictionary.\n",
    "# In our case, it returns a dictionary containing:\n",
    "\n",
    "# encoder_input\n",
    "\n",
    "# decoder_input\n",
    "\n",
    "# encoder_mask\n",
    "\n",
    "# decoder_mask\n",
    "\n",
    "# label\n",
    "\n",
    "# src_text\n",
    "\n",
    "# tgt_text\n",
    "\n",
    "# 2. DataLoader:\n",
    "\n",
    "# Purpose: The DataLoader is responsible for batching, shuffling,\n",
    "# and parallelizing the data loading process. It takes a Dataset object\n",
    "# as input and provides an iterable that yields batches of data.\n",
    "\n",
    "# How it Works:\n",
    "\n",
    "# The DataLoader receives a Dataset object (e.g., our BilingualDataset).\n",
    "\n",
    "# It uses the __len__ method of the Dataset to determine the size of the dataset.\n",
    "\n",
    "# It uses the __getitem__ method of the Dataset to fetch individual samples.\n",
    "\n",
    "# It groups these samples into batches. The batch_size argument\n",
    "# to the DataLoader controls how many samples are in each batch.\n",
    "\n",
    "# It can shuffle the data (if shuffle=True is passed to the DataLoader).\n",
    "# This is important to prevent the model from learning spurious correlations due to the order of the data.\n",
    "\n",
    "# It can load data in parallel using multiple worker processes (controlled by the num_workers argument). This can significantly speed up data loading, especially for large datasets.\n",
    "\n",
    "# Example of using DataLoader:\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have a 'raw_dataset' (e.g., a list of translation pairs)\n",
    "# and tokenizer_src, tokenizer_tgt defined.\n",
    "\n",
    "# 1. Create the Dataset\n",
    "bilingual_dataset = BilingualDataset(\n",
    "    raw_dataset,  # Your raw data\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    src_lang=\"en\",\n",
    "    tgt_lang=\"fr\",\n",
    "    seq_len=40  # Example sequence length\n",
    ")\n",
    "\n",
    "# 2. Create the DataLoader\n",
    "data_loader = DataLoader(\n",
    "    bilingual_dataset,\n",
    "    batch_size=32,  # Process 32 samples at a time\n",
    "    shuffle=True,  # Shuffle the data each epoch\n",
    "    num_workers=4   # Use 4 worker processes for parallel loading\n",
    ")\n",
    "\n",
    "# 3. Iterate through the DataLoader during training:\n",
    "for batch in data_loader:\n",
    "    encoder_input = batch[\"encoder_input\"]  # Shape: (batch_size, seq_len)\n",
    "    decoder_input = batch[\"decoder_input\"]  # Shape: (batch_size, seq_len)\n",
    "    encoder_mask = batch[\"encoder_mask\"]    # Shape: (batch_size, 1, 1, seq_len)\n",
    "    decoder_mask = batch[\"decoder_mask\"]    # Shape: (batch_size, 1, seq_len, seq_len)\n",
    "    label = batch[\"label\"]                # Shape: (batch_size, seq_len)\n",
    "\n",
    "    # Move data to the GPU if available\n",
    "    encoder_input = encoder_input.to(device)\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    encoder_mask = encoder_mask.to(device)\n",
    "    decoder_mask = decoder_mask.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    # Now you can feed these tensors to your Transformer model\n",
    "#     # ... your model forward pass here ...\n",
    "# content_copy\n",
    "# download\n",
    "# Use code with caution.\n",
    "# Python\n",
    "\n",
    "# Key Relationship:\n",
    "\n",
    "# The DataLoader uses the BilingualDataset to get data. The DataLoader\n",
    "# doesn't know how the data is prepared; it just calls the __getitem__ \n",
    "# method of the BilingualDataset and handles the batching, shuffling, and parallel loading.\n",
    "\n",
    "# In Summary:\n",
    "\n",
    "# The BilingualDataset is responsible for accessing and preparing individual data samples.\n",
    "\n",
    "# The DataLoader is responsible for organizing these samples into \n",
    "# batches and providing them to the training loop efficiently.\n",
    "\n",
    "# The DataLoader provides the batched data in a format suitable for\n",
    "# the Transformer model. The encoder_input, decoder_input, encoder_mask, \n",
    "# and decoder_mask tensors that the DataLoader yields are the input to the Transformer.\n",
    "\n",
    "# Now that we've clarified the role of the DataLoader, are you ready to \n",
    "# move on to the Transformer model architecture itself (embedding, positional \n",
    "# encoding, attention, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for idx, sample in enumerate(samples):\n",
    "        print(f\"Sample {idx + 1}:\")\n",
    "        print(f\"ID: {sample['id']}\")\n",
    "        print(f\"Source: {sample['translation']['en']}\")  # Assuming 'translation' is a dict with a 'en' key for source\n",
    "        print(f\"Target: {sample['translation']['it']}\")  # Assuming 'it' is the target language\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 1:\n",
    "- **ID**: 0\n",
    "- **Source**: Source: Project Gutenberg\n",
    "- **Target**: Source: [www.liberliber.it/Audiobook available here](www.liberliber.it/Audiobook available here)\n",
    "\n",
    "---\n",
    "\n",
    "Sample 2:\n",
    "- **ID**: 1\n",
    "- **Source**: Jane Eyre\n",
    "- **Target**: Jane Eyre\n",
    "\n",
    "---\n",
    "\n",
    "Sample 3:\n",
    "- **ID**: 2\n",
    "- **Source**: Charlotte Bronte\n",
    "- **Target**: Charlotte Brontë\n",
    "\n",
    "---\n",
    "\n",
    "Sample 4:\n",
    "- **ID**: 3\n",
    "- **Source**: CHAPTER I\n",
    "- **Target**: PARTE PRIMA\n",
    "\n",
    "---\n",
    "\n",
    "Sample 5:\n",
    "- **ID**: 4\n",
    "- **Source**: There was no possibility of taking a walk that day.\n",
    "- **Target**: I. In quel giorno era impossibile passeggiare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max_Sequence_Lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](PicturesPart2/Max_length_Size.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
