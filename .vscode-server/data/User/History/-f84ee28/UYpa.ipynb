{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input: [4, 9, 0, 8, 0, 6]\n",
      "Encoder Mask: [[[1, 1, 0, 1, 0, 1]]]\n",
      "Original Attention Scores:\n",
      "tensor([[ 1.4993,  0.1686, -1.1113, -1.0623, -0.3536, -1.9742],\n",
      "        [-0.9570,  0.5242, -0.6316, -0.0038,  1.1087, -0.1498],\n",
      "        [-0.1731,  1.6809, -0.0410,  0.0450, -0.2899,  0.1282],\n",
      "        [-1.8678, -0.0308, -0.7427,  0.4536, -0.7229, -1.9170],\n",
      "        [-1.0089, -0.8133,  1.4476, -1.2438, -0.1860,  0.3591],\n",
      "        [-1.1489,  2.2726, -0.1667, -1.9493, -1.5931, -0.9043]])\n",
      "Masked Attention Scores:\n",
      "tensor([[ 1.4993e+00,  1.6858e-01, -1.0000e+09, -1.0623e+00, -1.0000e+09,\n",
      "         -1.9742e+00],\n",
      "        [-9.5704e-01,  5.2424e-01, -1.0000e+09, -3.8383e-03, -1.0000e+09,\n",
      "         -1.4978e-01],\n",
      "        [-1.7306e-01,  1.6809e+00, -1.0000e+09,  4.4996e-02, -1.0000e+09,\n",
      "          1.2824e-01],\n",
      "        [-1.8678e+00, -3.0787e-02, -1.0000e+09,  4.5358e-01, -1.0000e+09,\n",
      "         -1.9170e+00],\n",
      "        [-1.0089e+00, -8.1331e-01, -1.0000e+09, -1.2438e+00, -1.0000e+09,\n",
      "          3.5905e-01],\n",
      "        [-1.1489e+00,  2.2726e+00, -1.0000e+09, -1.9493e+00, -1.0000e+09,\n",
      "         -9.0435e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define sequence length and vocab size\n",
    "seq_len = 6\n",
    "vocab_size = 10  # Assume tokens are integers from 0 to 9\n",
    "pad_token = 0  # Padding token is represented as 0\n",
    "\n",
    "# Step 1: Create random encoder input with some padding\n",
    "encoder_input = torch.randint(1, vocab_size, (seq_len,))  # Random tokens (excluding pad_token)\n",
    "encoder_input[2] = pad_token  # Manually add padding at index 2\n",
    "encoder_input[4] = pad_token  # Manually add padding at index 4\n",
    "\n",
    "print(\"Encoder Input:\", encoder_input.tolist())\n",
    "\n",
    "# Step 2: Create encoder mask\n",
    "encoder_mask = (encoder_input != pad_token).unsqueeze(0).unsqueeze(0).int()  # (1, 1, seq_len)\n",
    "\n",
    "print(\"Encoder Mask:\", encoder_mask.tolist())\n",
    "\n",
    "# Step 3: Generate dummy attention scores (seq_len x seq_len)\n",
    "attn_scores = torch.randn(seq_len, seq_len)  # Random attention scores\n",
    "\n",
    "print(\"Original Attention Scores:\")\n",
    "print(attn_scores)\n",
    "\n",
    "# Step 4: Apply mask (set masked positions to a very negative number)\n",
    "masked_attn_scores = attn_scores.masked_fill(encoder_mask.squeeze(0).squeeze(0) == 0, -1e9)\n",
    "\n",
    "print(\"Masked Attention Scores:\")\n",
    "print(masked_attn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Input: [3, 7, 0, 5, 9]\n",
      "Padding Mask: [[1, 1, 0, 1, 1]]\n",
      "Causal Mask:\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "Final Decoder Mask:\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def causal_mask(seq_len):\n",
    "    \"\"\"Creates a causal mask (upper triangular mask)\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))  # Lower triangular matrix (float by default)\n",
    "    return mask.unsqueeze(0).int()  # Convert to int\n",
    "\n",
    "# Example decoder input (with padding tokens)\n",
    "seq_len = 5\n",
    "pad_token = 0\n",
    "decoder_input = torch.tensor([3, 7, 0, 5, 9])  # Random sequence with a padding token at index 2\n",
    "\n",
    "# Padding mask: (1, seq_len)\n",
    "padding_mask = (decoder_input != pad_token).unsqueeze(0).int()\n",
    "\n",
    "# Causal mask: (1, seq_len, seq_len) (converted to int)\n",
    "causal_mask_tensor = causal_mask(seq_len)\n",
    "\n",
    "# Apply bitwise AND to combine padding mask and causal mask\n",
    "decoder_mask = padding_mask.unsqueeze(-1) & causal_mask_tensor  # Shape (1, seq_len, seq_len)\n",
    "\n",
    "print(\"Decoder Input:\", decoder_input.tolist())\n",
    "print(\"Padding Mask:\", padding_mask.tolist())\n",
    "print(\"Causal Mask:\\n\", causal_mask_tensor.squeeze(0))\n",
    "print(\"Final Decoder Mask:\\n\", decoder_mask.squeeze(0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
