{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZR6Nml9PmH7I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pDWO41YAYb3_"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert d_model%num_heads == 0 ,\"d_model must be divisible by num_heads\"\n",
        "        # Initialization dimension\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model , d_model)\n",
        "        self.W_k = nn.Linear(d_model , d_model)\n",
        "        self.W_v = nn.Linear(d_model , d_model)\n",
        "        self.W_o = nn.Linear(d_model , d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) \n",
        "        print(\"Shape of energy si \")\n",
        "        print(attn_scores.shape )\n",
        "        \n",
        "        attn_scores=attn_scores/ math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim = -1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_head(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_head(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask = None):\n",
        "        Q = self.split_head(self.W_q(Q))\n",
        "        K = self.split_head(self.W_k(K))\n",
        "        V = self.split_head(self.W_v(V))\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        output = self.W_o(self.combine_head(attn_output))\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of energy si \n",
            "torch.Size([2, 8, 10, 10])\n",
            "torch.Size([2, 10, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Define dimensions\n",
        "batch_size = 2\n",
        "seq_length = 10\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "# Create random tensors for Q, K, V\n",
        "Q = torch.randn(batch_size, seq_length, d_model)  # (2, 10, 512)\n",
        "K = torch.randn(batch_size, seq_length, d_model)  # (2, 10, 512)\n",
        "V = torch.randn(batch_size, seq_length, d_model)  # (2, 10, 512)\n",
        "\n",
        "# Initialize the multi-head attention module\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# Pass through the attention layer\n",
        "output = mha(Q, K, V)\n",
        "\n",
        "# Output shape should be (batch_size, seq_length, d_model)\n",
        "print(output.shape)  # Expected: torch.Size([2, 10, 512])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "        \n",
        "        # One word converted into 512 dims and 512 distrubute into 8 heads so one head will get 512/8= 64 dims\n",
        "        # 64 dims of q,k,v will go through the linear transformation which is a neural network \n",
        "        # after this each 64 dims query will go dot product with 64 dims of keys 64 dot 64 = attention value \n",
        "        # Example attention scores for first head might look like:\n",
        "        \n",
        "            #         .....\n",
        "            # Query shape is torch.Size([1, 10, 8, 64])\n",
        "            # Key shape is torch.Size([1, 10, 8, 64])\n",
        "            # Attention Score will be torch.Size([1, 10, 8, 8])\n",
        "            \n",
        "            # for every Q there is 64 dims\n",
        "            # for every k there is 64 dims\n",
        "            \n",
        "            # so when we apply dot product we get a 64.dot(64) we get a single value \n",
        "            # so output attention will be  torch.Size([1, 10, 8, 8])\n",
        "            # bwqd->bwkd \n",
        "            # after transpose bwqd->bwdk\n",
        "            # after dot product \n",
        "            # bwqk\n",
        "        \n",
        "            #        i*64    am*64    a*64    cat*64\n",
        "            # i*64    [0.2  0.3  0.1  0.4]\n",
        "            # am*64   [0.1  0.4  0.2  0.3]\n",
        "            # a*64    [0.1  0.2  0.3  0.4]\n",
        "            # cat*64  [0.3  0.2  0.1  0.4]\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        # after this we this we apply softmax to get the probality(sum of all equal to 1) of a query with \n",
        "        # respect to keys \n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask=None):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0] \n",
        "        # N No of batches\n",
        "        # \n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "        # Number of charater orr words\n",
        "\n",
        "        \n",
        "        values = self.values(values)  # (N, value_len, embed_size)\n",
        "        keys = self.keys(keys)  # (N, key_len, embed_size)\n",
        "        queries = self.queries(query)  # (N, query_len, embed_size)\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "        Q=queries\n",
        "        K=keys\n",
        "        V=values\n",
        "        \n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.embed_size)\n",
        "        \n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
        "      \n",
        "      \n",
        "        attn_probs = torch.softmax(attn_scores, dim = -1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Query si \n",
            "torch.Size([1, 10, 8, 64])\n",
            "Shape of energy si \n",
            "torch.Size([1, 10, 8, 64])\n",
            "Shape of energy si \n",
            "torch.Size([1, 10, 8, 8])\n",
            "Shape of \n",
            "torch.Size([1, 10, 8, 8])\n",
            "Values of  \n",
            "torch.Size([1, 10, 8, 64])\n",
            "torch.Size([1, 10, 8, 64])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Define dimensions\n",
        "batch_size = 1\n",
        "seq_length = 10\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "# Create random tensors for Q, K, V\n",
        "Q = torch.randn(batch_size, seq_length, d_model)  # (2, 10, 512)\n",
        "K = torch.randn(batch_size, seq_length, d_model)  # (2, 10, 512)\n",
        "V = torch.randn(batch_size, seq_length, d_model)  # (2, 10, 512)\n",
        "\n",
        "# Initialize the multi-head attention module\n",
        "mha = SelfAttention(d_model, num_heads)\n",
        "\n",
        "# Pass through the attention layer\n",
        "output = mha(Q, K, V)\n",
        "\n",
        "# Output shape should be (batch_size, seq_length, d_model)\n",
        "print(output.shape)  # Expected: torch.Size([2, 10, 512])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask=None):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, embed_size)\n",
        "        keys = self.keys(keys)  # (N, key_len, embed_size)\n",
        "        queries = self.queries(query)  # (N, query_len, embed_size)\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "\n",
        "        print(\"Shape of Query si \")\n",
        "        print(queries.shape )\n",
        "        print(\"Shape of energy si \")\n",
        "        print(keys.shape )\n",
        "        # energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        energy = torch.matmul(queries,keys.transpose(-2, -1))\n",
        "        print(\"Shape of energy si \")\n",
        "        print(energy.shape )\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy: (N, words, query_len, key_len)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        \n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=-1)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        print(\"Shape of \")\n",
        "        print(attention.shape )\n",
        "        print(\"Values of  \")\n",
        "        print(values.shape )\n",
        "        # out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
        "        out = torch.matmul(attention, values)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        # out = self.fc_out(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxbw7_DVYdj3"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "\n",
        "#         Initialization\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cloudspace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
