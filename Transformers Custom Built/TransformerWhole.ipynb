{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Self-Attention\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attn(x, x, x)[0]\n",
    "\n",
    "# Masked Self-Attention (for Decoder)\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device)\n",
    "        return self.attn(x, x, x, attn_mask=mask)[0]\n",
    "\n",
    "# Cross-Attention\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        return self.attn(x, memory, memory)[0]\n",
    "\n",
    "# Encoder Block\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.self_attn = SelfAttention(d_model, nhead)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.norm1(src + self.self_attn(src))\n",
    "        src = self.norm2(src + self.dropout(self.linear2(torch.relu(self.linear1(src)))))\n",
    "        return src\n",
    "\n",
    "# Decoder Block\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.masked_attn = MaskedSelfAttention(d_model, nhead)\n",
    "        self.cross_attn = CrossAttention(d_model, nhead)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        tgt = self.norm1(tgt + self.masked_attn(tgt))\n",
    "        tgt = self.norm2(tgt + self.cross_attn(tgt, memory))\n",
    "        tgt = self.norm3(tgt + self.dropout(self.linear2(torch.relu(self.linear1(tgt)))))\n",
    "        return tgt\n",
    "\n",
    "# Full Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([TransformerDecoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, src)\n",
    "        \n",
    "        return self.fc_out(tgt)\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = 30000\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "\n",
    "# Instantiate model\n",
    "model = Transformer(vocab_size, d_model, nhead, num_layers)\n",
    "\n",
    "# Dummy input (batch_size=32, seq_len=10)\n",
    "src = torch.randint(0, vocab_size, (32, 10))  # Encoder input\n",
    "tgt = torch.randint(0, vocab_size, (32, 10))  # Decoder input\n",
    "\n",
    "output = model(src, tgt)\n",
    "print(\"Output shape:\", output.shape)  # Expected: (32, 10, 30000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, src, max_len=50, start_token=2, end_token=3):\n",
    "    \"\"\"\n",
    "    Autoregressive text generation using the Transformer model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained Transformer model.\n",
    "        src: Input sequence (encoder input) of shape (1, seq_len).\n",
    "        max_len: Maximum length of generated text.\n",
    "        start_token: Token to start decoding.\n",
    "        end_token: Token to stop decoding.\n",
    "    \n",
    "    Returns:\n",
    "        Generated token sequence (tensor).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get model device\n",
    "    \n",
    "    # Ensure input is on the correct device\n",
    "    src = src.to(device)\n",
    "    \n",
    "    # Prepare encoder input - embed and add positional encoding\n",
    "    src_emb = model.embedding(src) * math.sqrt(model.embedding.embedding_dim)\n",
    "    src_emb = model.positional_encoding(src_emb)\n",
    "    \n",
    "    # Run through encoder to get memory\n",
    "    memory = src_emb\n",
    "    for layer in model.encoder_layers:\n",
    "        memory = layer(memory)\n",
    "    \n",
    "    # Initialize decoder input with start token\n",
    "    tgt = torch.tensor([[start_token]], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate sequence token by token\n",
    "    for _ in range(max_len):\n",
    "        # Embed current target sequence\n",
    "        tgt_emb = model.embedding(tgt) * math.sqrt(model.embedding.embedding_dim)\n",
    "        tgt_emb = model.positional_encoding(tgt_emb)\n",
    "        \n",
    "        # Decode step-by-step\n",
    "        decoded = tgt_emb\n",
    "        for layer in model.decoder_layers:\n",
    "            decoded = layer(decoded, memory)\n",
    "        \n",
    "        # Get prediction for the next token (last position only)\n",
    "        logits = model.fc_out(decoded[:, -1:])  # Shape: (1, 1, vocab_size)\n",
    "        next_token_logits = logits.squeeze(1)  # Shape: (1, vocab_size)\n",
    "        \n",
    "        # Get most likely token\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)  # Shape: (1, 1)\n",
    "        \n",
    "        # Append token to sequence\n",
    "        tgt = torch.cat([tgt, next_token], dim=1)\n",
    "        \n",
    "        # Stop if end token is generated\n",
    "        if next_token.item() == end_token:\n",
    "            break\n",
    "            \n",
    "    return tgt.squeeze(0)  # Remove batch dimension"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sele",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
