{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer library Coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class UrduTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, \n",
    "                 dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Positional encoding\n",
    "        position = torch.arange(0, 5000).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(5000, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x, src_mask=None, src_key_padding_mask=None):\n",
    "        # x shape: (batch, seq)\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.pos_encoder(x, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Output projection\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# Create model with PyTorch transformer\n",
    "vocab_size = 30000  # Urdu vocabulary size\n",
    "model = UrduTransformer(vocab_size)\n",
    "\n",
    "# Dummy input (batch_size, seq_length)\n",
    "dummy_input = torch.randint(0, vocab_size, (32, 10))\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)  # Expected: (32, 10, 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Transformer Encoder Block\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = self.norm1(src + src2)\n",
    "        src2 = self.linear2(F.relu(self.linear1(src)))\n",
    "        src = self.norm2(src + self.dropout(src2))\n",
    "        return src\n",
    "\n",
    "# Full Transformer Model\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = 30000  # Urdu vocabulary size\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "\n",
    "# Instantiate model\n",
    "model = CustomTransformer(vocab_size, d_model, nhead, num_layers)\n",
    "\n",
    "# Dummy input\n",
    "dummy_input = torch.randint(0, vocab_size, (10, 32))  # (seq_length, batch_size)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected: (seq_length, batch_size, vocab_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sele",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
