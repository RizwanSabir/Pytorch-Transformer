{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 n_segments,\n",
    "                 max_len,\n",
    "                 embed_dim,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.seg_embed = nn.Embedding(n_segments, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.pos_inp = torch.tensor([i for i in range(max_len)],)\n",
    "\n",
    "    def forward(self, seq, seg):\n",
    "        embed_val = self.tok_embed(seq) + self.seg_embed(seg) + self.pos_embed(self.pos_inp)\n",
    "        embed_val = self.drop(embed_val)\n",
    "        return embed_val\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 n_segments,\n",
    "                 max_len,\n",
    "                 embed_dim,\n",
    "                 n_layers,\n",
    "                 attn_heads,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = BERTEmbedding(vocab_size, n_segments, max_len, embed_dim, dropout)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embed_dim, attn_heads, embed_dim*4)\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, n_layers)\n",
    "\n",
    "    def forward(self, seq, seg):\n",
    "        out = self.embedding(seq, seg)\n",
    "        out = self.encoder_block(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    VOCAB_SIZE = 30000\n",
    "    N_SEGMENTS = 3\n",
    "    MAX_LEN = 512\n",
    "    EMBED_DIM = 768\n",
    "    N_LAYERS = 12\n",
    "    ATTN_HEADS = 12\n",
    "    DROPOUT = 0.1\n",
    "\n",
    "    sample_seq = torch.randint(high=VOCAB_SIZE, size=[MAX_LEN,])\n",
    "    sample_seg = torch.randint(high=N_SEGMENTS, size=[MAX_LEN,])\n",
    "\n",
    "    embedding = BERTEmbedding(VOCAB_SIZE, N_SEGMENTS, MAX_LEN, EMBED_DIM, DROPOUT)\n",
    "    embedding_tensor = embedding(sample_seq, sample_seg)\n",
    "    print(embedding_tensor.size())\n",
    "\n",
    "    bert = BERT(VOCAB_SIZE, N_SEGMENTS, MAX_LEN, EMBED_DIM, N_LAYERS, ATTN_HEADS, DROPOUT)\n",
    "    out = bert(sample_seq, sample_seg)\n",
    "    print(out.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sele",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
