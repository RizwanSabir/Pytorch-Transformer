{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input: [4, 9, 0, 8, 0, 6]\n",
      "Encoder Mask: [[[1, 1, 0, 1, 0, 1]]]\n",
      "Original Attention Scores:\n",
      "tensor([[ 1.4993,  0.1686, -1.1113, -1.0623, -0.3536, -1.9742],\n",
      "        [-0.9570,  0.5242, -0.6316, -0.0038,  1.1087, -0.1498],\n",
      "        [-0.1731,  1.6809, -0.0410,  0.0450, -0.2899,  0.1282],\n",
      "        [-1.8678, -0.0308, -0.7427,  0.4536, -0.7229, -1.9170],\n",
      "        [-1.0089, -0.8133,  1.4476, -1.2438, -0.1860,  0.3591],\n",
      "        [-1.1489,  2.2726, -0.1667, -1.9493, -1.5931, -0.9043]])\n",
      "Masked Attention Scores:\n",
      "tensor([[ 1.4993e+00,  1.6858e-01, -1.0000e+09, -1.0623e+00, -1.0000e+09,\n",
      "         -1.9742e+00],\n",
      "        [-9.5704e-01,  5.2424e-01, -1.0000e+09, -3.8383e-03, -1.0000e+09,\n",
      "         -1.4978e-01],\n",
      "        [-1.7306e-01,  1.6809e+00, -1.0000e+09,  4.4996e-02, -1.0000e+09,\n",
      "          1.2824e-01],\n",
      "        [-1.8678e+00, -3.0787e-02, -1.0000e+09,  4.5358e-01, -1.0000e+09,\n",
      "         -1.9170e+00],\n",
      "        [-1.0089e+00, -8.1331e-01, -1.0000e+09, -1.2438e+00, -1.0000e+09,\n",
      "          3.5905e-01],\n",
      "        [-1.1489e+00,  2.2726e+00, -1.0000e+09, -1.9493e+00, -1.0000e+09,\n",
      "         -9.0435e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define sequence length and vocab size\n",
    "seq_len = 6\n",
    "vocab_size = 10  # Assume tokens are integers from 0 to 9\n",
    "pad_token = 0  # Padding token is represented as 0\n",
    "\n",
    "# Step 1: Create random encoder input with some padding\n",
    "encoder_input = torch.randint(1, vocab_size, (seq_len,))  # Random tokens (excluding pad_token)\n",
    "encoder_input[2] = pad_token  # Manually add padding at index 2\n",
    "encoder_input[4] = pad_token  # Manually add padding at index 4\n",
    "\n",
    "print(\"Encoder Input:\", encoder_input.tolist())\n",
    "\n",
    "# Step 2: Create encoder mask\n",
    "encoder_mask = (encoder_input != pad_token).unsqueeze(0).unsqueeze(0).int()  # (1, 1, seq_len)\n",
    "\n",
    "print(\"Encoder Mask:\", encoder_mask.tolist())\n",
    "\n",
    "# Step 3: Generate dummy attention scores (seq_len x seq_len)\n",
    "attn_scores = torch.randn(seq_len, seq_len)  # Random attention scores\n",
    "\n",
    "print(\"Original Attention Scores:\")\n",
    "print(attn_scores)\n",
    "\n",
    "# Step 4: Apply mask (set masked positions to a very negative number)\n",
    "masked_attn_scores = attn_scores.masked_fill(encoder_mask.squeeze(0).squeeze(0) == 0, -1e9)\n",
    "\n",
    "print(\"Masked Attention Scores:\")\n",
    "print(masked_attn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Input: [3, 7, 0, 5, 9]\n",
      "Padding Mask: [[1, 1, 0, 1, 1]]\n",
      "Causal Mask:\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "Final Decoder Mask:\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "\n",
      "Original Attention Scores:\n",
      " tensor([[-2.0673,  1.2917,  0.9502,  1.6841,  0.3131],\n",
      "        [-1.6142,  0.4656,  0.3090, -0.8188, -0.0746],\n",
      "        [ 0.0351, -0.0609,  0.9087, -0.2348, -0.5654],\n",
      "        [ 0.0091,  2.3085,  0.2298,  0.4479,  0.9507],\n",
      "        [ 0.9271, -0.7019, -0.8267,  0.8278, -0.1082]])\n",
      "\n",
      "Masked Attention Scores:\n",
      " tensor([[-2.0673e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.6142e+00,  4.6559e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 9.0505e-03,  2.3085e+00,  2.2975e-01,  4.4795e-01, -1.0000e+09],\n",
      "        [ 9.2712e-01, -7.0191e-01, -8.2667e-01,  8.2778e-01, -1.0820e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def causal_mask(seq_len):\n",
    "    \"\"\"Creates a causal mask (upper triangular mask)\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))  # Lower triangular matrix (float by default)\n",
    "    return mask.unsqueeze(0).int()  # Convert to int\n",
    "\n",
    "# Example decoder input (with padding tokens)\n",
    "seq_len = 5\n",
    "pad_token = 0\n",
    "decoder_input = torch.tensor([3, 7, 0, 5, 9])  # Random sequence with a padding token at index 2\n",
    "\n",
    "# Padding mask: (1, seq_len)\n",
    "padding_mask = (decoder_input != pad_token).unsqueeze(0).int()\n",
    "\n",
    "# Causal mask: (1, seq_len, seq_len) (converted to int)\n",
    "causal_mask_tensor = causal_mask(seq_len)\n",
    "\n",
    "# Final decoder mask (1, seq_len, seq_len)\n",
    "decoder_mask = padding_mask.unsqueeze(-1) & causal_mask_tensor  # Shape (1, seq_len, seq_len)\n",
    "\n",
    "# Step 1: Generate random attention scores (seq_len x seq_len)\n",
    "attn_scores = torch.randn(seq_len, seq_len)  # Random values\n",
    "\n",
    "# Step 2: Apply decoder mask (set masked positions to a very negative number)\n",
    "masked_attn_scores = attn_scores.masked_fill(decoder_mask.squeeze(0) == 0, -1e9)\n",
    "\n",
    "# Print outputs\n",
    "print(\"Decoder Input:\", decoder_input.tolist())\n",
    "print(\"Padding Mask:\", padding_mask.tolist())\n",
    "print(\"Causal Mask:\\n\", causal_mask_tensor.squeeze(0))\n",
    "print(\"Final Decoder Mask:\\n\", decoder_mask.squeeze(0))\n",
    "print(\"\\nOriginal Attention Scores:\\n\", attn_scores)\n",
    "print(\"\\nMasked Attention Scores:\\n\", masked_attn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Attention Scores:\n",
      "tensor([[[[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784],\n",
      "          [-1.2345, -0.0431, -1.6047, -0.7521, -0.6866],\n",
      "          [-0.4934,  0.2415, -1.1109,  0.0915, -2.3169],\n",
      "          [-0.2168, -1.3847, -0.3957,  0.8034, -0.6216],\n",
      "          [-0.5920, -0.0631, -0.8286,  0.3309, -1.5576]]]])\n",
      "\n",
      "Mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True, False, False]])\n",
      "\n",
      "Softmax with -1e9 for masked values:\n",
      "tensor([[[[0.4993, 0.3217, 0.1789, 0.0000, 0.0000],\n",
      "          [0.2007, 0.6607, 0.1386, 0.0000, 0.0000],\n",
      "          [0.2759, 0.5753, 0.1488, 0.0000, 0.0000],\n",
      "          [0.4657, 0.1449, 0.3894, 0.0000, 0.0000],\n",
      "          [0.2868, 0.4868, 0.2264, 0.0000, 0.0000]]]])\n",
      "\n",
      "Softmax with 0 for masked values:\n",
      "tensor([[[[0.4360, 0.2809, 0.1562, 0.0635, 0.0635],\n",
      "          [0.0843, 0.2777, 0.0583, 0.2899, 0.2899],\n",
      "          [0.1449, 0.3022, 0.0782, 0.2374, 0.2374],\n",
      "          [0.2159, 0.0672, 0.1805, 0.2682, 0.2682],\n",
      "          [0.1408, 0.2390, 0.1111, 0.2545, 0.2545]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Dummy attention scores: shape (batch, heads, seq_len, seq_len)\n",
    "batch_size, heads, seq_len = 1, 1, 5\n",
    "attention_scores = torch.randn(batch_size, heads, seq_len, seq_len)\n",
    "\n",
    "# Create a binary mask where 1 means valid token and 0 means masked token.\n",
    "# For example, let's mask out the last two tokens (columns).\n",
    "mask = torch.tensor([[1, 1, 1, 0, 0]], dtype=torch.bool)  # shape: (seq_len,)\n",
    "mask = mask.unsqueeze(0).unsqueeze(0)  # shape: (batch, heads, seq_len)\n",
    "\n",
    "# Expand mask to match attention_scores shape along the last dimension\n",
    "mask = mask.expand(batch_size, heads, seq_len, seq_len)\n",
    "\n",
    "# --- Case 1: Using -1e9 for masked values ---\n",
    "attn_scores_neg_inf = attention_scores.clone()\n",
    "attn_scores_neg_inf.masked_fill_(mask == 0, -1e9)\n",
    "softmax_neg_inf = F.softmax(attn_scores_neg_inf, dim=-1)\n",
    "\n",
    "# --- Case 2: Using 0 for masked values ---\n",
    "attn_scores_zero = attention_scores.clone()\n",
    "attn_scores_zero.masked_fill_(mask == 0, 0)\n",
    "softmax_zero = F.softmax(attn_scores_zero, dim=-1)\n",
    "\n",
    "print(\"Original Attention Scores:\")\n",
    "print(attention_scores)\n",
    "print(\"\\nMask:\")\n",
    "print(mask[0, 0])  # print one example mask for clarity\n",
    "\n",
    "print(\"\\nSoftmax with -1e9 for masked values:\")\n",
    "print(softmax_neg_inf)\n",
    "\n",
    "print(\"\\nSoftmax with 0 for masked values:\")\n",
    "print(softmax_zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.514948844909668\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define parameters\n",
    "batch_size = 2  # Example batch size\n",
    "seq_length = 512  # Given sequence length\n",
    "vocab_size = 3000  # Given vocabulary size\n",
    "\n",
    "# Simulate model output (logits) and labels\n",
    "proj_output = torch.randn(batch_size, seq_length, vocab_size)  # Model logits\n",
    "label = torch.randint(0, vocab_size, (batch_size, seq_length))  # Ground truth labels\n",
    "\n",
    "# Implement CrossEntropyLoss from scratch\n",
    "def custom_cross_entropy_loss(logits, targets):\n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # Apply log softmax\n",
    "    nll_loss = -log_probs[range(log_probs.shape[0]), targets]  # Negative log likelihood loss\n",
    "    return nll_loss.mean()  # Mean loss\n",
    "\n",
    "# Reshape for loss computation\n",
    "proj_output_reshaped = proj_output.view(-1, vocab_size)  # Shape: (batch_size * seq_length, vocab_size)\n",
    "label_reshaped = label.view(-1)  # Shape: (batch_size * seq_length)\n",
    "\n",
    "# Compute loss using custom CrossEntropyLoss\n",
    "loss = custom_cross_entropy_loss(proj_output_reshaped, label_reshaped)\n",
    "\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sele",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
